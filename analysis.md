# Analysis

## Layer 6, Head 11
![Alt text](Attention_Layer6_Head11.png "a title")

Based on what we see in this diagram, we can see that this attention head has learned to pay attention to what word came before each word in the sentence. It makes sense since the context and meaning of a word often depends on what it's preceding word is

Example Sentences:
- we turned down a narrow lane and passed through a small [MASK].
- manila is one of the most [MASK] city

## Layer 2, Head 11
![Alt text](Attention_Layer2_Head11.png "a title")

Based on what we see in this diagram, we can see that the model is trying to figure out the relationship between the verb and the next words. I think it's trying to figure out for example, in what context is the verb being use in

Example:
"turned": "down a narrow lane" describes where and how the subject turned

Example Sentences:
- we turned down a narrow lane and passed through a small [MASK].
- manila is one of the most [MASK] city